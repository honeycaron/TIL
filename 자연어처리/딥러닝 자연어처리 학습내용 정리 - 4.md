# Text Classification
> 주어진 텍스트가 이미 정해진 카테고리 중 어디로 분류되어야 하는지 예측하는 작업

### MLP(Multi-Layer Perceptron)
- 각각의 문서를 고정 길이를 가지는 벡터로 변환 후, Vocab Size를 입력층의 뉴런 수로 가지는 MLP에 입력으로 사용
- MLP는 피드 포워드 신경망(Feed-Foward Neural Network)임

### RNN(Recurrent Neural Network)
- RNN은 피드 포워드 신경망에 시점이라는 개념을 도입함
- 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해
- 연속적인 시퀀스를 처리하고 내부에 정보를 지속하는 루프로 구성된 신경망
- 대표적으로 one-to-many, many-to-one, many-to-many가 있음
- 현재 시점인 hidden state 연산을 위해 직전 시점의 hidden state를 입력받아 과거의 정보를 기억함
- RNN은 시점이 길어지면 앞의 정보가 소실되는 '장기 의존성 문제'를 갖고 있기 때문에 기억력을 높인 LSTM(Long Short-Term Memory)를 사용함
#### LSTM
- 기존 RNN에 없었던 Cell State에 gate라는 구조를 통해 정보를 더하거나 빼는 등의 통제가 가능함
- 입력 게이트는 현재 정보를 기억하기 위한 게이트이며 삭제 게이트는 기억을 삭제하기 위한 게이트임
- 출력 게이트는 Hidden State를 연산함
#### GRU
- LSTM과 마찬가지로 장기 의존성 문제에 강건
- 3개의 게이트가 있었던 LSTM과 달리 업데이트 게이트, 리셋 게이트 2개를 사용하는 것이 특징임
#### Bidirectional RNN
- 역방향으로 입력을 참고하는 RNN을 추가하여 양방향으로 만들 수 있음
- 양방향 RNN은 앞의 문맥뿐만 아니라 뒤의 문맥까지 참고할 수 있다는 이점이 있음
### RNN Language Model
- 오직 n개의 고정된 길이만 입력받을 수 있는 NNLM와는 다르게 입력 길이가 고정되지 않음
- 아키텍처는 Embedding layer, Hidden layer, Output layer로 구성
- Teacher Forcing: 훈련 시에 실제값(Ground Truth)을 입력으로 사용하여 빠른 학습 가능
### CNN for Text Classification
- 이미지의 특징을 추출하는 CNN 기법을 텍스트 분류에도 적용 가능
- 1D Convolution: 오직 한 방향으로만 윈도우를 슬라이딩하여 겹치는 부분의 임베딩 행렬과 커널의 원소값을 곱해서 모두 더한 값을 출력한 후 Max-pooling으로 벡터에서 스칼라 값을 추출
- 매우 단순한 구조임에도 준수한 성능을 보임
- Character-level CNN for Text Classification: 오탈자, OOV, 신조어 등에 강건
