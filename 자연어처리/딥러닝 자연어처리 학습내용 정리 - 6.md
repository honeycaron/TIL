> 자연어처리는 크게 자연어 이해(NLU)와 자연어 생성(NLG)이 있음. 자연어 생성은 기계가 텍스트를 스스로 생성하는 영역으로 Image Caption, Chatbot, 그리고 **Neural Machine Translation**에 쓰임

Neural Machine Translation에 핵심 기술인 Sequence-To-Sequence에 대해서 배워보고자 함

# Sequence-to-Sequence
- Sequence-to-Sequence는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력함
- 내부적으로 인코더(NLU, 다대일 RNN)와 디코더(NLG, 다대다 RNN) 구조를 가지고 있음
- 인코더의 마지막 은닉 상태를 컨텍스트 벡터를 디코더 RNN 셀의 첫번째 은닉 상태로 사용
- 인코더와 디코더의 각 시점의 입력은 기본적으로 임베딩 벡터임
- 디코더는 RNN 언어 모델로 Teacher Forcing 비율을 정해서 Training 진행(비율이 높으면 학습이 빠르지만 과적합의 가능성이 있음)
- 테스트 단계에서는 Teacher Forcing 사용 안함
- 디코더는 매 시점마다 가장 높은 확률을 가지는 단어 선택 -> Greedy Decoding
- Greedy Decoding은 순간 잘못된 선택을 했더라도 결정을 취소할 수 없음 -> Beam Searching 사용
- Beam Searching: 매 시점마다 가장 확률이 높은 K개의 다음 단어를 선택, 다음 단계에서는 K*K개를 선택한 후, 후보군 중에서 가장 높은 K개의 후보군만 유지(해당 시점에서 누적 확률 순)

# etc
- Byte Pair Encoding, Unigram Language Model Tokenizer 등 Subword Tokenization을 통해 OOV(Out-Of-Vocabulary) 문제를 해결할 수 있음
- BLEU(Bilingual Evaluation Understudy) Score로 번역의 품질 평가 가능
	- BLEU는 언어에 구애받지 않고, 빠른 계산이 가능하지만 Score가 절대적인 Quality를 보장하지는 않음(문장의 의미, 문장의 구조 등)

#### *(참고) 기계 번역기의 서비스 과정
- 데이터 수집: 병렬 데이터 구매 또는 크롤링
- 데이터 정제(Cleaning) : 크롤링 된 데이터에는 반드시 노이즈가 있음(정렬 확인, 특수문자 제거 필요)
- 토크나이저 사용: Mecab 또는 SentencePiece 사용권장
- 데이터의 분리: 학습 데이터, 검증 데이터, 테스트 데이터 분리
- 모델 선정: seq2seq with Attention, Transformer 계열 등의 모델 선정
- 학습: 배치크기, learning rate 등과 같은 하이퍼파라미터 선정 후 GPU를 통해 학습
- 추론(테스트): Beam Search를이용하여 테스트
- 역토큰화(Detokenization): 번역된 문장을 역토큰화하여 자연스러운 문장 형태로 변환
- 성능 평가: BLEU, TER 등을 사용하여 모델을 평가 후 개선
- 모델 배포: 서버와 웹 서비스를 사용하여 실제 번역기를 서비스화
