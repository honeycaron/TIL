## Transformer
- 'Attention is All you need' 논문에서 제안함
- 기계 번역을 위해 탄생
- 인코더-디코더 구조를 여전히 유지
- 병렬 연산을 추구하며 RNN을 사용하지 않음
- 인코더와 디코더 블록이 N개 존재(논문에서는 6개)
- 임베딩 벡터에 Positional Encoding을 거친 후 입력으로 함(입력을 병렬로 받기 때문)
- Positinal Encoding은 사인 함수와 코사인 함수의 그래프를 통해서 위치 정보를 더함
- 트랜스포머는 세 종류의 어텐션이 존재
	- Encoder Self-Attention: Query = Key = Value
   	- Masked Decoder Self-Attention: Query = Key = Value
   	- Encoder-Decoder Attention: Query = Decoder 벡터 / Key,Value = Encoder 벡터
### Transformer Encoder: Self-Attention
- Query, Key, Value가 모두 동일한 경우를 뜻함
- 입력 문장 내의 단어들 간의 유사도를 구함
- 입력 벡터에서 가중치 행렬 곱으로 Q,K,V 벡터를 얻고 수행
	- 논문에서는 입력 벡터 차원 512, 가중치 행렬을 512 x 64로 해서 Q,K,V 벡터 차원은 64
- 스케일드 닷 프로덕트 어텐션을 통해 각각의 Q벡터가 각각의 K벡터에 대해서 스코어 연산 -> (문장 길이) X (문장 길이)
- Q,K의 유사도를 통해 Attention Score를 구한 후 Softmax를 통과시킨 후 V와 곱해서 Attention Value를 구함(이 때 < pad>는 굉장히 작은 음수값으로 masking 진행)
- 실제로는 벡터 간 연산이 아니라 행렬 연산으로 이루어짐
- Multi-Head Attention은 Attention Value를 구하는 과정을 병렬적으로 여러번 수행하여 마지막에 전부 연결함
- 연결한 행렬에 마지막으로 가중치 행렬을 한번 더 곱해줌
- 논문에서는 최종적으로 (문장 길이) x (64 X 8 = 512)의 Multi-head Attention Matrix를 얻음
### Transformer Encoder: Position-Wise Feed Forward Neural Network
- 단순 피드 포워드 신경망
- 은닉층에서 활성화 함수로 ReLU 함수 사용
- 논문에서는 은닉층 크기로 2,048 사용
- 최종 출력은 인코더의 입력 크기였던 (문장 길이) x 512가 보존되며 이 행렬은 다음 인코더 층의 입력으로 전달됨
### Add & Norm
- Add: Residual Connection -> 연산의 결과를 연산의 입력과 다시 더해주는 것. 트랜스포머에서는 서브층의 연산 결과를 입력과 다시 더함
- Norm: Layer Normalization -> Residual Connection Output의 각 벡터의 평균과 분산을 구해서 정규화 시켜줌
### Tranformer Decoder: Masked Self-Attention
- 트랜스포머 디코더의 Masked Self-Attention은 근본적으로 Sefl-Attention과 동일함
- 하지만 Attention Score Matrix에 직각 삼각형의 마스킹을 해준다는 차이점이 존재함
- 마스킹을 하는 이유는 앞의 단어가 뒤의 단어를 예측하도록 학습해야되기 때문임 -> 앞 단어가 뒷 단어를 미리 알면 안됨
### Tranformer Decoder: Encoder-Decoder Attention
- Query는 디코더 벡터이고 Key와 Value는 인코더 벡터임
- 어텐션 스코어 매트릭스는 (입력 문장) x (출력 문장) <-> 인코더에서는 (입력 문장) x (입력문장)
- Key에 < pad>가 있다면 그 열은 마스킹함


## Contextual Representation
- ELMo(Embeddings from Language Model): BiLM(양방향 언어 모델) 사용하여 Pre-trained Embedding으로 사용 -> 성능 Good, 다의어 처리 가능
- GPT(Generative Pre-Training, 2018): Train Deep(12-layer) Transformer LM -> Fine-tune on Classification Task



## BERT
> ELMo는 진정한 양방향이 아니다. GPT도 단방향 모델이다. 우리는 Fine-Tuning 방식을 취하되, 양방향을 실현할 것이다!
- Transformer의 Encoder를 갖고 만들었음
	- Token Embedding: Input Word Embedding
   	- Position Embedding: 트랜스포머 Positial Encoding의 대체
   	- Segment Embedding: 두 개의 문장을 구분하기 위한 Embedding
- Masked Language Model: 훈련 데이터의 단어 중 k%를 마스크로 변경하여 이를 예측하도록 함
	- k% 중에서도 80%는 마스크, 10%는 랜덤 단어, 나머지 10%는 원래 단어 그대로로 설정
- 다음 문장 예측 태스크를 도입
- BERT-BASE: 인코더 12층 / BERT-LARGE: 인코더 24층
- 실제로는 패딩 토큰을 0으로 마스킹하는 어텐션 마스크도 사용함
- CLS 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰
- BERT를 사전학습 시킨 후, Text Classification, NER, QA, NLI 등 다양한 태스크에 적용
