# Vectorization
- 원-핫 인코딩: 각 단어에 고유한 정수 인덱스를 부여하고, 해당 인덱스 원소는 1, 나머지는 0인 벡터
- DTM(Document Term Matrix): 각 단어는 고유한 정수 인덱스를 가지며, 해당 단어의 등장 횟수를 해당 인덱스의 값으로 가짐 (Bag of Words로 표현)
- TF-IDF: DTM에서 중요한 단어에 가중치를 주는 방식
	- 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단함
  	- log를 씌우는 이유는 희귀 단어들에 엄청난 가중치가 부여될 수 있기 때문
 	- 문서간 유사도를 구할 수 있음
    
# Machine Learning Basics
- 머신 러닝 모델 기본 과정: 
> **가설 수립 - 손실 함수 정의 - 손실 함수 최소화를 위한 학습 알고리즘 설계**
- 손실 함수(비용 함수)를 최소화하는 매개변수(w와 b)를 찾는 것이 목표 -> 옵티마이저 사용

종류|목표|가설|손실 함수
---|---|---|---
선형 회귀|연속적인 값 예측|y=wx+b|평균 제곱 오차(MSE) 
로지스틱 회귀|이진 분류| y=sigmoid(wx+b)|Cross-Entropy 
소프트맥스 회귀|다중 클래스 분류|y=softmax(wx+b)|Cross-Entropy

- 소프트맥스 회귀는 각 카테고리를 원-핫 인코딩해야 함
- 은닉층: 입력층과 출력층 사이에 층을 추가, 보통 데이터가 많을 때 사용
- 텍스트 분류: 주어진 텍스트가 이미 정의된 카테고리 중 어디로 분류되어야 하는지 예측
(예: 언론사 뉴스 카테고리 분류, 사용자 리뷰 긍정/부정 분류)
