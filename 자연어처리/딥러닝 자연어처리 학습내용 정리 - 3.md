# Language Model

- Language Model(LM): 단어 시퀀스에 확률을 할당하는 모델 -> 주어진 단어 시퀀스 다음에 어떤 단어가 등장해야 자연스러운지를 판단함

- N-gram Language Model: 다음 단어의 등장 확률을 앞의 n-1개의 단어로 구함. (예: Bigram(n=2)은 오직 이전 단어 한개만 의존하여 확률 추정)

- N-gram 언어 모델은 앞의 n-1개의 단어만 고려하므로 n이 너무 작으면 장기의존성 문제가 발생. _(장기의존성: 앞(초반)에 있는 단어가 중요한데, n이 너무 작으면 앞에 있는 단어가 고려되지 않을 수 있는 문제)_
반면, n이 커지면 훈련 데이터 내에 카운트가 안되서 제대로 된 모델링이 되지 않음(sparsity problem)
**--> 이런 문제를 해결하고자 인공 신경만 언어모델(NNLM)을 사용!**
(인공 신경만 언어 모델은 희소 문제를 Word Embedding으로 해결함)

## Neural Network Language Model
- NNLM(Neural Network Language Model: n개의 이전 단어들로부터 n+1 단어를 예측하는 모델. 신경망의 입력을 원-핫 벡터로 받음.

- Projection Layer: i번째 인덱스에 1의 값을 가지는 원-핫 벡터와 가중치 W행력의 곱으로 W행렬의 i번째 행을 그대로 읽어옴(Lookup). 이 연산을 Lookup Table이라고 부름. Weight는 있지만 Bias는 없고 활성화 함수를 사용하지 않는 것(Linear)이 특징임. 각 원-핫 벡터가 lookup table을 거쳐서 concatenate됨.
--> Projection Layer 이후, 하나의 Hidden Layer(nonlinear)을 거치고 Output Layer에서 실제값인 다음 단어 원-핫 벡터로부터 loss를 구하고 학습됨(projection layer의 embedding table이 학습됨).

- Training 과정에서 유사한 용도로 사용되는 단어들은 유사한 단어 벡터값을 가지게 된다. (예: 강아지와 고양이)

## Word Embedding
- 워드 임베딩을 통해 단어 벡터 간의 유사도를 구할 수 있고 크게 두가지로 구분된다.

 번호 | 종류 | 설명 
----|----|----
1|랜덤 초기화 임베딩|NNLM과 마찬가지로 랜덤값을 가지고 오차를 구하는 과정에서 embedding table을 학습. Task에 맞도록 embedding vector값이 최적화됨.
2|사전 훈련된 임베딩|이미 방대한 데이터로 학습시킨 알고리즘(Word2Vec,FastText,Glove 등)을 Task의 입력으로 사용.

- 랜덤 초기화 임베딩은 Projection Layer 대신 Embedding Layer을 계산.

### Word2Vec
- Word2Vec: 단어의 의미를 반영한 임베딩 벡터를 만드는 대표적인 방법. 

- CBoW: 주변 단어들로부터 중심 단어를 예측. 윈도우 크기가 2면 앞뒤 2단어로 중심단어를 예측함. Projection layer에서 모든 embedding vector들의 평균값을 구하여 M차원의 벡터를 얻음. M차원의 벡터를 가중치 행렬 W'와 곱하여 소프트맥스 함수 통과.

- Skip-gram: 중심 단어로부터 주변단어를 예측. 입력층,투사층,출력층 3개의 층으로 신경망이 구성됨. 역시 소프트맥스 함수를 통과.

_* 참고: NNLM과 다르게 word2vec에는 hidden layer가 존재하지 않음_

**실제로는 CBoW와 Skip-gram을 구현할 때, 단어 집합 크기가 크기때문에 softmax + cross entropy 연산하면 너무 느림 --> SGNS을 사용!**

> 비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다 --> SGNS: 윈도우 사이즈 내에 있는 단어들의 벡터를 유사하게 만들자! 

- Skip-Gram With Negtive Sampling(SGNS): 다중 클래스 분류(중심단어 --> 주변단어)가 아니라 이진 분류(중심단어,주변단어 --> 1)로 바꿔서 연산량을 줄임. 즉, 중심 단어와 주변 단어를 입력했을 때 이 두 단어가 이웃관계인지 예측하는 것. 거짓을 의미하는(주변단어가 아닌) 샘플들도 추가해주어야 하기 때문에 Negative Sampling이라고 함. 2개의 Embedding Table(중심단어 + 주변단어)의 내적으로부터 실제값(1 또는 0)을 예측하고 역전파를 통해 두 개의 테이블을 업데이트함. 

_* 참고: 논문에서는 Negative Sampling의 비율로 5-20을 최적의 숫자로 정의하고 있음(데이터가 방대하면 2-5로도 충분함)_

# ETC
- linear: 활성화 함수가 없음
- non-linear: 활성화 함수가 있음
- 활성화 함수: sigmoid, softmax, relu(은닉층에 자주 쓰임)
- 벡터의 내적: 벡터의 유사도를 구하는 방법
- 두 벡터의 내적값이 크다 : 두 벡터가 유사하다
- 작은 윈도우 크기일수록 상호 교환할 수 정도의 높은 유사도를 가짐(반의어 포함)
